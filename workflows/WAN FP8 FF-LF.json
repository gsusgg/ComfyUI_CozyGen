{
  "2": {
    "inputs": {
      "vae_name": "WAN\\wan_2.1_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "3": {
    "inputs": {
      "shift": 8.000000000000002,
      "model": [
        "6",
        0
      ]
    },
    "class_type": "ModelSamplingSD3",
    "_meta": {
      "title": "ModelSamplingSD3"
    }
  },
  "4": {
    "inputs": {
      "shift": 8.000000000000002,
      "model": [
        "5",
        0
      ]
    },
    "class_type": "ModelSamplingSD3",
    "_meta": {
      "title": "ModelSamplingSD3"
    }
  },
  "5": {
    "inputs": {
      "sage_attention": "auto",
      "model": [
        "20",
        0
      ]
    },
    "class_type": "PathchSageAttentionKJ",
    "_meta": {
      "title": "Patch Sage Attention KJ"
    }
  },
  "6": {
    "inputs": {
      "sage_attention": "auto",
      "model": [
        "12",
        0
      ]
    },
    "class_type": "PathchSageAttentionKJ",
    "_meta": {
      "title": "Patch Sage Attention KJ"
    }
  },
  "9": {
    "inputs": {
      "samples": [
        "19",
        0
      ],
      "vae": [
        "2",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "12": {
    "inputs": {
      "lora_name": "WAN\\Wan2.2-Lightning_I2V-A14B-4steps-lora_LOW_fp16.safetensors",
      "strength_model": 1.0000000000000002,
      "model": [
        "22",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "14": {
    "inputs": {
      "unet_name": "WAN\\wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors",
      "weight_dtype": "default"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "Load Diffusion Model"
    }
  },
  "15": {
    "inputs": {
      "unet_name": "WAN\\wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors",
      "weight_dtype": "default"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "Load Diffusion Model"
    }
  },
  "16": {
    "inputs": {
      "clip_name": "umt5_xxl_fp8_e4m3fn_scaled.safetensors",
      "type": "wan",
      "device": "default"
    },
    "class_type": "CLIPLoader",
    "_meta": {
      "title": "Load CLIP"
    }
  },
  "17": {
    "inputs": {
      "value": [
        "18",
        0
      ]
    },
    "class_type": "UnloadAllModels",
    "_meta": {
      "title": "UnloadAllModels"
    }
  },
  "18": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 46021538309736,
      "steps": 4,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "start_at_step": 0,
      "end_at_step": 2,
      "return_with_leftover_noise": "enable",
      "model": [
        "4",
        0
      ],
      "positive": [
        "32",
        0
      ],
      "negative": [
        "32",
        1
      ],
      "latent_image": [
        "32",
        2
      ]
    },
    "class_type": "KSamplerAdvanced",
    "_meta": {
      "title": "KSampler (Advanced)"
    }
  },
  "19": {
    "inputs": {
      "add_noise": "disable",
      "noise_seed": 740819805698133,
      "steps": 4,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "start_at_step": 2,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "model": [
        "3",
        0
      ],
      "positive": [
        "32",
        0
      ],
      "negative": [
        "32",
        1
      ],
      "latent_image": [
        "17",
        0
      ]
    },
    "class_type": "KSamplerAdvanced",
    "_meta": {
      "title": "KSampler (Advanced)"
    }
  },
  "20": {
    "inputs": {
      "lora_name": "WAN\\Wan2.2-Lightning_I2V-A14B-4steps-lora_HIGH_fp16.safetensors",
      "strength_model": 1.0000000000000002,
      "model": [
        "21",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "21": {
    "inputs": {
      "lora_name": "WAN\\lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors",
      "strength_model": 1,
      "model": [
        "14",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "22": {
    "inputs": {
      "lora_name": "WAN\\lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors",
      "strength_model": 1,
      "model": [
        "15",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "23": {
    "inputs": {
      "value": 0
    },
    "class_type": "ImpactInt",
    "_meta": {
      "title": "Height"
    }
  },
  "24": {
    "inputs": {
      "value": 0
    },
    "class_type": "ImpactInt",
    "_meta": {
      "title": "Width"
    }
  },
  "25": {
    "inputs": {
      "width": [
        "30",
        1
      ],
      "height": [
        "30",
        2
      ],
      "upscale_method": "lanczos",
      "keep_proportion": "stretch",
      "pad_color": "0, 0, 0",
      "crop_position": "center",
      "divisible_by": 2,
      "device": "cpu",
      "image": [
        "45",
        0
      ]
    },
    "class_type": "ImageResizeKJv2",
    "_meta": {
      "title": "Resize Image v2"
    }
  },
  "26": {
    "inputs": {
      "text": "The woman lifts the flame above her head as the camera zooms out.",
      "clip": [
        "16",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive Prompt)"
    }
  },
  "27": {
    "inputs": {
      "frame_rate": 24,
      "loop_count": 0,
      "filename_prefix": "wan/09-09-2025/wan",
      "format": "video/h264-mp4",
      "pix_fmt": "yuv420p",
      "crf": 19,
      "save_metadata": true,
      "trim_to_audio": false,
      "pingpong": false,
      "save_output": true,
      "images": [
        "9",
        0
      ]
    },
    "class_type": "VHS_VideoCombine",
    "_meta": {
      "title": "Video Combine 🎥🅥🅗🅢"
    }
  },
  "28": {
    "inputs": {
      "text": "色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走",
      "clip": [
        "16",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative Prompt)"
    }
  },
  "30": {
    "inputs": {
      "width": [
        "40",
        0
      ],
      "height": [
        "40",
        1
      ],
      "upscale_method": "lanczos",
      "keep_proportion": "stretch",
      "pad_color": "0, 0, 0",
      "crop_position": "center",
      "divisible_by": 2,
      "device": "cpu",
      "image": [
        "44",
        0
      ]
    },
    "class_type": "ImageResizeKJv2",
    "_meta": {
      "title": "Resize Image v2"
    }
  },
  "32": {
    "inputs": {
      "width": [
        "25",
        1
      ],
      "height": [
        "25",
        2
      ],
      "length": 81,
      "batch_size": 1,
      "positive": [
        "26",
        0
      ],
      "negative": [
        "28",
        0
      ],
      "vae": [
        "2",
        0
      ],
      "start_image": [
        "30",
        0
      ],
      "end_image": [
        "25",
        0
      ]
    },
    "class_type": "WanFirstLastFrameToVideo",
    "_meta": {
      "title": "WanFirstLastFrameToVideo"
    }
  },
  "34": {
    "inputs": {
      "image": "kontext_00005_.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Last Frame"
    }
  },
  "35": {
    "inputs": {
      "image": "kontext_00003_.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "First Frame"
    }
  },
  "37": {
    "inputs": {
      "value": [
        "44",
        0
      ]
    },
    "class_type": "UnloadAllModels",
    "_meta": {
      "title": "UnloadAllModels"
    }
  },
  "38": {
    "inputs": {
      "output": "",
      "input": [
        "40",
        0
      ]
    },
    "class_type": "Display Int (rgthree)",
    "_meta": {
      "title": "Display Int (rgthree)"
    }
  },
  "39": {
    "inputs": {
      "output": "",
      "input": [
        "40",
        1
      ]
    },
    "class_type": "Display Int (rgthree)",
    "_meta": {
      "title": "Display Int (rgthree)"
    }
  },
  "40": {
    "inputs": {
      "max_width": 720,
      "max_height": 720,
      "round_to": 64,
      "allow_upscale": true,
      "batch_policy": "first",
      "image": [
        "37",
        0
      ]
    },
    "class_type": "AspectFitSizeChooser",
    "_meta": {
      "title": "Aspect Fit → Size (INTs)"
    }
  },
  "41": {
    "inputs": {},
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "42": {
    "inputs": {
      "start": 121,
      "length": 1,
      "image": [
        "9",
        0
      ]
    },
    "class_type": "ImageFromBatch+",
    "_meta": {
      "title": "🔧 Image From Batch"
    }
  },
  "43": {
    "inputs": {
      "filename_prefix": "CozyGen/output",
      "images": [
        "42",
        0
      ]
    },
    "class_type": "CozyGenOutput",
    "_meta": {
      "title": "CozyGen Output"
    }
  },
  "44": {
    "inputs": {
      "param_name": "Image Input 1",
      "base64_image": ""
    },
    "class_type": "CozyGenImageInput",
    "_meta": {
      "title": "CozyGen Image Input"
    }
  },
  "45": {
    "inputs": {
      "param_name": "Image Input 2",
      "base64_image": ""
    },
    "class_type": "CozyGenImageInput",
    "_meta": {
      "title": "CozyGen Image Input"
    }
  }
}